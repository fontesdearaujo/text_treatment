import re
import os
from nltk import word_tokenize

def split_by_break(texto):
        texto = re.sub(r'[“”]', '"', texto)
        return [sentenca.strip() for sentenca in texto.splitlines()]

def remove_empty_entrance(texto):
        texto = list(filter(lambda x: x != '', texto))
        return texto

def find_questions(q):
        questions = re.findall('(?<=[\?\.\!]\s)[^\?\n\.]+?\?', q)
        return questions

def paragrafos(texto):
       tam = []
       tam.append(len(texto))
       return sum(tam)

def main():
    path = "...."
    os.chdir(path)
    filelist = []

    for root, dirs, files in os.walk(path):
        for file in files:
            if file.endswith(".txt"):
                filelist.append(path+file)
    
    questions = []
    result = []
    for aux in filelist:
        file = open(aux, 'r', encoding='utf-8')
        text = file.read()

        result = result + remove_empty_entrance(split_by_break(text))
        questions = questions + find_questions(text)
    
    print(questions)
    print('Total de paragrafos: ' + str(paragrafos(result)))
    print('Total de perguntas: '+ str(len(questions)))

if __name__ == "__main__":
    main()
    
